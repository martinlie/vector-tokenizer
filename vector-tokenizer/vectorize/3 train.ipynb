{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b5589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "from jax import value_and_grad\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from helper_funcs import generate, masked_fill, loss_fn, get_token_batch, encode, decode\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8352b8b1",
   "metadata": {},
   "source": [
    "## Parameter Selection\n",
    "\n",
    "The Parameters used below are a scaled down version of GPT-2. GPT-2 has 4 different sizes, small, medium, large and xl. This GPT-2 could be considered an extra-small version. Note that these models may not be able to fit into RAM on your device. The exact specifications of the different sized models are shown below:\n",
    "\n",
    "### GPT-2 Small\n",
    "- n_embed: 768\n",
    "- block_size: 1024\n",
    "- num_heads: 12\n",
    "- num_layers: 12\n",
    "- vocab_size: 50257 (uses Tiktoken vocab)\n",
    "\n",
    "### GPT-2 Medium\n",
    "- n_embed: 1024\n",
    "- block_size: 1024\n",
    "- num_heads: 16\n",
    "- num_layers: 24\n",
    "- vocab_size: 50257 (uses Tiktoken vocab)\n",
    "\n",
    "### GPT-2 Large\n",
    "- n_embed: 1280\n",
    "- block_size: 1024\n",
    "- num_heads: 20\n",
    "- num_layers: 36\n",
    "- vocab_size: 50257 (uses Tiktoken vocab)\n",
    "\n",
    "### GPT-2 XL\n",
    "- n_embed: 1600\n",
    "- block_size: 1024\n",
    "- num_heads: 25\n",
    "- num_layers: 48\n",
    "- vocab_size: 50257 (uses Tiktoken vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8354c917",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embed = 32 # Number of embedding dimensions\n",
    "batch_size = 4 # How many independent sequences will we process in parallel?\n",
    "block_size = 480 # What is the maximum context length for predictions?\n",
    "num_heads = 4 # Number of heads in the multi-headed block\n",
    "num_layers = 6 # Number of transformer decoder blocks\n",
    "drop_rate = 0.1 # Dropout rate for regularization\n",
    "#token_id_col='vector_id'\n",
    "token_id_col='cluster'\n",
    "\n",
    "rng_key = jax.random.PRNGKey(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1eb3d6",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cb6cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.pickle', 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "\n",
    "#vocab_size = len(loaded_data['vocab'])\n",
    "#print(vocab_size)\n",
    "\n",
    "X = pd.read_parquet(\"X.parquet\")\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8e816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "df_slice = X.iloc[100:224]\n",
    "\n",
    "v = encode(df_slice, token_id_col=token_id_col)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195b5e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode(v, X, token_id_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050ce095",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = X[token_id_col].max()\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78766d3",
   "metadata": {},
   "source": [
    "## Build the Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced26c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a9def1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2(vocab_size, n_embed, block_size, num_heads, num_layers, drop_rate)\n",
    "dummy_x = jnp.zeros(shape=(batch_size, block_size), dtype=jnp.uint16)\n",
    "variables = model.init(rng_key, dummy_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0999641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.apply(variables, dummy_x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0096dd2",
   "metadata": {},
   "source": [
    "## Time Series Generation Pre-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4270d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_seq = jnp.zeros(shape=(1,1), dtype=jnp.uint16)\n",
    "max_new_tokens = 24\n",
    "learning_rate=1e-4\n",
    "\n",
    "#generated_indices = generate(variables, model.apply, index_seq, rng_key, vocab_size, 1, block_size, max_new_tokens)\n",
    "#generated_indices = list(np.array(generated_indices[0]))\n",
    "#decode(generated_indices, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d401f6",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b06ee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adamw(learning_rate=learning_rate)\n",
    "opt_state = optimizer.init(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69450594",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "train_ids = X[token_id_col].to_numpy()\n",
    "losses = []\n",
    "\n",
    "pbar = tqdm(range(epochs))\n",
    "for step in pbar:\n",
    "    rng_key, subkey = jax.random.split(rng_key)\n",
    "    xb, yb = get_token_batch(train_ids, subkey, batch_size, block_size)\n",
    "\n",
    "    loss, grads = value_and_grad(loss_fn, argnums=(0))(\n",
    "        variables, \n",
    "        model.apply,\n",
    "        xb, \n",
    "        yb\n",
    "    )\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, variables)\n",
    "    variables = optax.apply_updates(variables, updates)\n",
    "\n",
    "    if math.isnan(loss):\n",
    "        break\n",
    "\n",
    "    losses.append(loss)\n",
    "    \n",
    "    pbar.set_description(f\"Epoch: {step}, Loss: {loss :.4f}\")\n",
    "\n",
    "# Save model\n",
    "model_file = {\n",
    "    \"epochs\": epochs,\n",
    "    \"epoch\": step,\n",
    "    \"model\": model,\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"block_size\": block_size,\n",
    "    \"variables\": variables,\n",
    "    \"losses\": losses,\n",
    "    \"opt_state\": opt_state,\n",
    "    \"learning_rate\": learning_rate,\n",
    "}\n",
    "\n",
    "with open('model.pickle', 'wb') as f:\n",
    "    pickle.dump(model_file, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d28e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(losses)\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028add0a",
   "metadata": {},
   "source": [
    "## Time Series Generation Post-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8e0c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key, subkey = jax.random.split(rng_key)\n",
    "max_new_tokens = 480\n",
    "t = int(random.randint(rng_key, shape=(), minval=0, maxval=len(X)-(3*max_new_tokens)))\n",
    "\n",
    "# Select a random batch (t)\n",
    "X_test = X.iloc[t:t+max_new_tokens]\n",
    "Y_test = X.iloc[t+max_new_tokens+1:t+max_new_tokens+max_new_tokens+1].reset_index()\n",
    "x = [encode(X_test, token_id_col)]\n",
    "index_seq = jnp.array(x)\n",
    "\n",
    "print(index_seq.shape)\n",
    "print(index_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ef7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_key, subkey = jax.random.split(rng_key)\n",
    "generated_indices = generate(variables, model.apply, index_seq, rng_key, vocab_size, 1, block_size, max_new_tokens)\n",
    "generated_indices = list(np.array(generated_indices[0]))\n",
    "Y = decode(generated_indices, X, token_id_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a3cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(Y.index, Y['Power demand'], label=\"Predicted\")\n",
    "plt.plot(Y_test.index, Y_test['Power demand'], label=\"Actual\")\n",
    "plt.title('Predicted vs actual')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
